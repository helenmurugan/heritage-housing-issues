{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Cleaning Notebook Version 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Data Exploration of missing data using ProfileReport\n",
        "* Initial correlation analysis and PPS study\n",
        "* Detailed evaluation of missing data\n",
        "* Imputing strategies for missing data\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/house_prices_records.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate cleaned Train and Test sets, both saved under outputs/datasets/cleaned\n",
        "\n",
        "## Conclusions\n",
        "* Drop variable - ['EnclosedPorch', 'WoodDeckSF', 'GarageYrBlt']\n",
        "* Median Imputer - ['LotFrontage', '2ndFlrSF', 'MasVnrArea']\n",
        "* Mean Imputer - 'BedroomAbvGr'\n",
        "* Categorical Imputer - ['GarageFinish', 'BsmtFinType1']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/house_prices_records.csv\"))\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a list of the columns with missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "vars_with_missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Generate pandas profile report of the variables with missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "if vars_with_missing_data:\n",
        "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"There are no variables with missing data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Observations from pandas profile report:\n",
        "* Almost 10% of total data is missing\n",
        "* There are nine variables in total with missing data:\n",
        "    * Seven numeric variables have data missing\n",
        "    * Two categorical variables have data missing\n",
        "* Variable 'WoodDeckSF' and 'EnclosedPorch' have 90.7% and 89.4% missing data respectively. Given the context, it is possible that the reason for the missing data is that the houses did not have a wood deck and/or porch, however, this is speculation and the data in that case should have been an entry of 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Correlation and PPS Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Correlation analysis is performed on raw data to identify inconsistencies or anomalies that require attention. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code taken from CI Churnometer project is designed to visualise the correlation and Predictive Power Score (PPS) matrices of the dataframe, in the following way:\n",
        "* heatmap_corr: generates a heatmap to visualise the correlation matrix of the df. It masks values below a certain threshold and annotates the heatmap with correlation coefficients.\n",
        "* heatmap_pps: generates a heatmap to visualise the PPS matrix of the df. It masks values below a certain threshold and annotates the heatmap with PPS scores.\n",
        "* CalculateCorrAndPPS: calculates the Pearson and Spearman correlation matrices, as well as the PPS matrix of the input df.\n",
        "* DisplayCorrAndPPS: displays the correlation matrices (Pearson and Spearman) and the PPS matrix using heatmaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "\n",
        "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
        "                    linewidth=0.5\n",
        "                    )\n",
        "        axes.set_yticklabels(df.columns, rotation=0)\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
        "                         linewidth=0.05, linecolor='grey')\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    df_corr_spearman = df.corr(method=\"spearman\")\n",
        "    df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "    print(pps_score_stats.round(3))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
        "                      figsize=(20, 12), font_annot=8):\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
        "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "    print(\"It evaluates monotonic relationship \\n\")\n",
        "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate Correlations and Power Predictive Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* From the PPS interquartile range we can see the majority of values are between 0 and 0.066, which indicates there is a concentration of features with moderate predictive power.\n",
        "* A maximum PPS of 0.702 suggests that there is at least one feature in the dataset that has a strong association with another feature.\n",
        "* A threshold of 0.4 was chosen initially to display features with moderate correlation or predictive power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson = df_corr_pearson,\n",
        "                  df_corr_spearman = df_corr_spearman, \n",
        "                  pps_matrix = pps_matrix,\n",
        "                  CorrThreshold = 0.4, PPS_Threshold =0.4,\n",
        "                  figsize=(12,10), font_annot=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observations from heatmap analyses:\n",
        "* Sales price has moderate to strong monotonic relationship (Spearman correlation) with 12 house attributes.\n",
        "* Sales price has moderate to strong linear relationship (Pearson correlation) with 9 house attributes.\n",
        "* OverallQual and GrLivArea have the strongest predictive power for the target variable.\n",
        "* Moderate to strong multicollinearity correlations exist between several features, as expected, such as YearBuilt and GarageYrBlt, and 1stFlrSF and TotalBsmtSF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning\n",
        "## Assessing Missing Data Levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Custom function taken from CI Churnometer project to display missing data levels in a df, it shows the absolute levels, relative levels and data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "    missing_data_absolute = df.isnull().sum()\n",
        "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
        "    df_missing_data = (pd.DataFrame(\n",
        "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
        "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
        "                                   \"DataType\": df.dtypes}\n",
        "                                    )\n",
        "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
        "                          .query(\"PercentageOfDataset > 0\")\n",
        "                          )\n",
        "\n",
        "    return df_missing_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Load original dataset and check missing data levels.\n",
        "* Note - if dataset with encoded variables was used to evaluate missing data, the encoded variables appeared not to have missing data, due to the way they were encoded. However, we know that some of the categorical variables do have missing data (see pandas report above). We load original df to work around this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Spreadsheet Summary\n",
        "After careful consideration of the missing data levels and analysis of variables in the context of the business case, the following approaches will be tried to deal with missing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enclosed Porch - 90.68% missing - drop variable\n",
        "\n",
        "* The 'EnclosedPorch' variable will be dropped.\n",
        "* We could assume that the reason for the missing data is that the properties in these observations had no enclosed porch. However, given that this would affect such a large portion of the data, it is reasonable to drop the variable.\n",
        "* As such a large portion of the data is missing, this variable would not add much value to the model.\n",
        "\n",
        "### Wood Deck Square Feet - 89.38% missing - drop variable\n",
        "\n",
        "* The 'WoodDeckSF'variable will be dropped.\n",
        "* We could assume that the reason for the missing data is that the properties in these observations had no wood deck. However, given that this would affect such a large portion of the data, it is reasonable to drop the variable.\n",
        "* As such a large portion of the data is missing, this variable would not add much value to the model.\n",
        "\n",
        "### Lot Frontage - 17.74% missing - median imputer\n",
        "* The missing data in 'LotFrontage' will be imputed with the median value of 69.\n",
        "* The median was chosen because the data is slightly skewed with a large spread. \n",
        "* Mean imputation with the value of 70 cis an alternative approach that could have been taken since the data is quite close to a normal distribution. Given the skew median imputation is likely to represent the missing data more accurately.\n",
        "* It was considered that maybe the data is missing because the house does not have a front lot (front door of property opens directly onto street). However, closer inspection of the data shows that no observations recorded 0, and therefore it can be assumed that the data is missing for another reason, and that all Iowan houses have a front lot of some size.\n",
        "\n",
        "### Garage Finish - 11.10% missing - categorical imputer with 'unf'\n",
        "* The missing data in 'GarageFinish' will be categorically imputed with 'unf' to represent unfinished.\n",
        "* It was considered that maybe the data is missing because the houses have no garages. However, on closer inspection of the 'GarageArea' variable we can see that the variable has no missing data and 5.5% recorded as 0 suggesting that 5.5% houses in the dataset have no garage. 'GarageFinish' has 5.6% recorded as 'None' meaning no garage and therefore the houses with no garage are accounted for in the 'GarageFinish' data.\n",
        "* Since we know that the missing data is for houses that do have garages we will impute with the most common category which is 'unf' and is significantly more common than any other category in this dataset.\n",
        "\n",
        "### Basement Finish Type 1 - 7.81% missing - categorical imputer with 'unf'\n",
        "* The missing data in 'BsmtFinType1' will be categorically imputed with 'unf' to represent unfinished.\n",
        "* It was considered that maybe the data is missing because the houses have no basement. However, on closer inspection of the 'TotalBsmtSF' variable we can see that the variable has no missing data and 2.5% recorded as 0 suggesting that 2.5% houses in the dataset have no basement. 'BsmtFinType1' has 2.3% recorded as 'None' meaning no basement and therefore the houses with no basement are mostly accounted for in the 'BsmtFinType1' data.\n",
        "* Since we know that the missing data is for houses that do have basements we will impute with a category. Possible categories to impute with are 'unf' (unfinished) or 'glq' (good living quarters), which are the top most common categories and unfortunately have similar levels of 29.4% and 28.6% respectively, meaning we cannot confidently impute with either one of these categories.\n",
        "* It was considered whether we could drop the rows with missing data, however, we would lose 7.81% of our data which is too much data to lose.\n",
        "* It was considered whether we could drop the variable altogether. This could be a reasonable approach, since we know that 'BsmtFinType1' has only a weak correlation with the target variable.\n",
        "* Taking all of these factors into consideration, it was decided better not to lose any data and to impute with the most common category of 'unf'.\n",
        "\n",
        "### Bedrooms Above Grade - 6.78% missing - mean imputer\n",
        "* The missing data in 'BedroomAbvGr' will be imputed with the mean value of 2.87.\n",
        "* The mean was chosen as the distribution of this data is close to normal distribution (standard deviation is 0.8).\n",
        "* Median imputation would also have been a reasonable approach to take (median is 3) with this data.\n",
        "\n",
        "### 2nd Floor Square Feet - 5.89% missing - median imputer\n",
        "* The missing data for the variable '2ndFlrSF' will be imputed with the value of 0.\n",
        "* 0 is the median value for this data.\n",
        "* 0 holds significance as it represents houses without a second storey.\n",
        "\n",
        "### Garage Year Built - 5.55% missing - drop variable\n",
        "* The 'GarageYrBlt' variable will be dropped.\n",
        "* There is a problem with the collection of this data. Houses with no garage do not have a year that the garage was built. Even choosing 0 would not work, as that suggests year 0.\n",
        "* On closer inspection of the 'GarageArea' variable, which has no missing data we can see that 5.5% have recorded 0, suggesting 5.5% of houses have no garage. This precentage matches with the missing data in 'GarageYrBlt'.\n",
        "* We cannot impute a year for a garage that does not exist.\n",
        "* It was considered to drop rows with missing data, but then we would drop all of the houses with no garage which would result in an inaccurate dataset.\n",
        "* Although 'GarageYrBlt' has a strong correlation to the target variable, the dataset also has variables for the year the house was built and the year of remodelling, both of which were strongly correlated to the garage year built and therefore dropping 'GarageYrBlt' should not affect the model too much because other related house attributes capture the age of the property.\n",
        "\n",
        "### Masonry Veneer Area - 0.55% missing - median imputer\n",
        "* The missing data for the variable 'MasVnrArea' will be imputed with the value of 0.\n",
        "* 0 is the median value for this data.\n",
        "* 0 holds significance as it represents houses without masonry veneer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dealing with Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DataCleaningEffect() is a custom function from CI's Feature Engine module.\n",
        "\n",
        "Function objective: assess the effect of cleaning the data, when:\n",
        "* mean, median or arbitrary value imputation is performed on a numerical variable.\n",
        "* 'Missing' or the most frequest category is used to impute categorical variables\n",
        "\n",
        "Parameters: \n",
        "* df_original: data not cleaned\n",
        "* df_cleaned: cleaned data\n",
        "* variables_applied_with_method: variables where you applied a given method\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def DataCleaningEffect(df_original,df_cleaned,variables_applied_with_method):\n",
        "\n",
        "  flag_count=1 # Indicate plot number\n",
        "  \n",
        "  # distinguish between numerical and categorical variables\n",
        "  categorical_variables = df_original.select_dtypes(exclude=['number']).columns \n",
        "\n",
        "  # scan over variables, \n",
        "    # first on variables that you applied the method\n",
        "    # if the variable is a numerical plot, a histogram if categorical plot a barplot\n",
        "  for set_of_variables in [variables_applied_with_method]:\n",
        "    print(\"\\n=====================================================================================\")\n",
        "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
        "    print(f\"{set_of_variables} \\n\\n\")\n",
        "  \n",
        "\n",
        "    for var in set_of_variables:\n",
        "      if var in categorical_variables:  # it is categorical variable: barplot\n",
        "        \n",
        "        df1 = pd.DataFrame({\"Type\":\"Original\",\"Value\":df_original[var]})\n",
        "        df2 = pd.DataFrame({\"Type\":\"Cleaned\",\"Value\":df_cleaned[var]})\n",
        "        dfAux = pd.concat([df1, df2], axis=0)\n",
        "        fig , axes = plt.subplots(figsize=(15, 5))\n",
        "        sns.countplot(hue='Type', data=dfAux, x=\"Value\",palette=['#432371',\"#FAAE7B\"])\n",
        "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.legend() \n",
        "\n",
        "      else: # it is numerical variable: histogram\n",
        "\n",
        "        fig , axes = plt.subplots(figsize=(10, 5))\n",
        "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True,element=\"step\", ax=axes)\n",
        "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True,element=\"step\", ax=axes)\n",
        "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.legend() \n",
        "\n",
        "      plt.show()\n",
        "      flag_count+= 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning Summary\n",
        "* Drop - ['EnclosedPorch', 'WoodDeckSF', 'GarageYrBlt']\n",
        "* Median Imputer - ['LotFrontage', '2ndFlrSF', 'MasVnrArea']\n",
        "* Mean Imputer - ['BedroomAbvGr', ]\n",
        "* Categorical Imputer - ['GarageFinish', 'BsmtFinType1']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Drop Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "variables_method = ['EnclosedPorch', 'WoodDeckSF', 'GarageYrBlt' ]\n",
        "variables_method\n",
        "\n",
        "imputer = DropFeatures(features_to_drop=variables_method)\n",
        "df_method = imputer.fit_transform(df)\n",
        "df_method.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Median Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Assess the affect of median imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "variables_method = ['LotFrontage', '2ndFlrSF', 'MasVnrArea']\n",
        "variables_method\n",
        "\n",
        "\n",
        "imputer = MeanMedianImputer(imputation_method='median', variables=variables_method)\n",
        "                            \n",
        "df_method = imputer.fit_transform(df)\n",
        "\n",
        "DataCleaningEffect(df_original=df,\n",
        "                   df_cleaned=df_method,\n",
        "                   variables_applied_with_method=variables_method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Assess the affect of mean imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_method = ['BedroomAbvGr']\n",
        "variables_method\n",
        "\n",
        "\n",
        "imputer = MeanMedianImputer(imputation_method='mean', variables=variables_method)\n",
        "                            \n",
        "df_method = imputer.fit_transform(df)\n",
        "\n",
        "DataCleaningEffect(df_original=df,\n",
        "                   df_cleaned=df_method,\n",
        "                   variables_applied_with_method=variables_method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "\n",
        "variables_method = ['GarageFinish', 'BsmtFinType1']\n",
        "variables_method\n",
        "\n",
        "imputer = CategoricalImputer(imputation_method='missing', fill_value='Unf', variables=variables_method)\n",
        "df_method = imputer.fit_transform(df)\n",
        "\n",
        "DataCleaningEffect(df_original=df,\n",
        "                   df_cleaned=df_method,\n",
        "                   variables_applied_with_method=variables_method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* After comparison of the original and cleaned data, we accept the imputation methods, and apply them to the train and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split Train Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['SalePrice'],\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=0)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Apply data cleaning techniques to handle missing data in train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop Variables\n",
        "variables_method = ['EnclosedPorch', 'WoodDeckSF', 'GarageYrBlt']\n",
        "imputer = DropFeatures(features_to_drop=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = imputer.transform(TrainSet), imputer.transform(TestSet)\n",
        "\n",
        "# Median Imputer\n",
        "variables_method = ['LotFrontage', '2ndFlrSF', 'MasVnrArea']\n",
        "imputer = MeanMedianImputer(imputation_method='median', variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)\n",
        "\n",
        "# Mean Imputer\n",
        "variables_method = ['BedroomAbvGr']\n",
        "imputer = MeanMedianImputer(imputation_method='mean', variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)\n",
        "\n",
        "# Categorical Imputer\n",
        "variables_method = ['GarageFinish', 'BsmtFinType1']\n",
        "imputer = CategoricalImputer(imputation_method='missing', fill_value='Unf', variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Evaluate missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing_data = EvaluateMissingData(TrainSet)\n",
        "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
        "df_missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Conclusions\n",
        "* Missing data has been eliminated from the dataset using the following methods.\n",
        "\n",
        "    * Drop variable - ['EnclosedPorch', 'WoodDeckSF', 'GarageYrBlt']\n",
        "    * Median Imputer - ['LotFrontage', '2ndFlrSF', 'MasVnrArea']\n",
        "    * Mean Imputer - ['BedroomAbvGr', ]\n",
        "    * Categorical Imputer - ['GarageFinish', 'BsmtFinType1']\n",
        "\n",
        "* Data quality improvements:\n",
        "    * For variables '2ndFlrSF', 'MasVnrArea', 'Lotfrontage' and 'BedroomAbvGr' the shape of the distribution remained the same after data cleaning, with only slight increase in peak height due to imputation.\n",
        "* Data quality concerns:\n",
        "    * The categorical imputations have changed the relative distribution of the categories by a small amount.\n",
        "    * Dropping the 'GargeYrBlt' variable seems quite drastic given a relatively small amount of missing data, however, it was considered the best approach after careful consideration of all factors (as discussed above).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "* Detailed correlation study\n",
        "* Generate data visualisations to answer client business requirement.\n",
        "* Use cleaned Train and Test sets for feature engineering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/cleaned') # create a folder for the data output\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "df_cleaned.to_csv(f\"outputs/datasets/cleaned/house_prices_records_cleaned.csv\",index=False)\n",
        "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)\n",
        "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
